{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import wandb \n",
    "from torchinfo import summary\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from loaddata import load_cesm2_by_period\n",
    "from UNet import UNet\n",
    "from trainer import Trainer, EarlyStopper\n",
    "from utils import DotDict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name\n",
    "exp = 'test'\n",
    "out_dir = f'../output/{exp}'\n",
    "data_dir = '../data/cesm2'\n",
    "\n",
    "# Hyperparameters\n",
    "hp = {\n",
    "    'vnames': ['sst', 'ssh', 'taux'],\n",
    "    'lat_slice': (-50, 50),\n",
    "    'target_vname': 'sst',\n",
    "    'target_grid': '2x2',\n",
    "    'target_lat_slice': (-10, 10),\n",
    "    'target_lon_slice': (120, 290),\n",
    "    't1_dist_f': 'target_distance_scaled_0_-3_-6_-9.nc',\n",
    "    # 't1_dist_f': 'target_distance_0_-3_-6_-9.nc',\n",
    "    # 't1_dist_f': 'target_distance.nc',\n",
    "    'lead': 12,\n",
    "    'month': 1,\n",
    "    'periods': {\n",
    "        'train': (1865, 1958),\n",
    "        'val': (1959, 1985),\n",
    "        'test': (1986, 1998),\n",
    "    },\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-5,\n",
    "    'model': 'UNet',\n",
    "    'attention': False,\n",
    "    'is_res': False,\n",
    "    'depth': 4,\n",
    "    'init_ch': 256,\n",
    "    'n_sub': 188,\n",
    "    'n_analog': 30,\n",
    "    'n_epochs': 60,\n",
    "}\n",
    "hp = DotDict(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9400, 3, 21, 72])\n",
      "torch.Size([9400, 11, 86])\n",
      "CPU times: user 2.37 s, sys: 13.7 s, total: 16.1 s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load data\n",
    "(datasets, dataloaders, \n",
    " t0_library, t0_mask, \n",
    " t1_library) = load_cesm2_by_period(data_dir, **hp)\n",
    "\n",
    "print(t0_library.shape)\n",
    "print(t1_library.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 21, 72)\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# dimension\n",
    "x, _, _ = datasets['train'][0]\n",
    "x_shape = tuple(x.shape)\n",
    "n_channels = x.shape[0]\n",
    "print(x_shape)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "t0_library = t0_library.to(device)\n",
    "t0_mask = t0_mask.to(device)\n",
    "t1_library = t1_library.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "UNet                                               [16, 3, 21, 72]           --\n",
      "├─Encoder: 1-1                                     [16, 256, 21, 72]         --\n",
      "│    └─ModuleList: 2-7                             --                        (recursive)\n",
      "│    │    └─Down: 3-1                              [16, 256, 21, 72]         598,272\n",
      "│    └─MaxPool2d: 2-2                              [16, 256, 10, 36]         --\n",
      "│    └─ModuleList: 2-7                             --                        (recursive)\n",
      "│    │    └─Down: 3-2                              [16, 512, 10, 36]         3,542,016\n",
      "│    └─MaxPool2d: 2-4                              [16, 512, 5, 18]          --\n",
      "│    └─ModuleList: 2-7                             --                        (recursive)\n",
      "│    │    └─Down: 3-3                              [16, 1024, 5, 18]         14,161,920\n",
      "│    └─MaxPool2d: 2-6                              [16, 1024, 2, 9]          --\n",
      "│    └─ModuleList: 2-7                             --                        (recursive)\n",
      "│    │    └─Down: 3-4                              [16, 2048, 2, 9]          56,635,392\n",
      "├─Decoder: 1-2                                     [16, 256, 21, 72]         --\n",
      "│    └─ModuleList: 2-8                             --                        --\n",
      "│    │    └─Up: 3-5                                [16, 1024, 5, 18]         36,707,328\n",
      "│    │    └─Up: 3-6                                [16, 512, 10, 36]         9,178,624\n",
      "│    │    └─Up: 3-7                                [16, 256, 21, 72]         2,295,552\n",
      "├─Conv2d: 1-3                                      [16, 3, 21, 72]           771\n",
      "├─Sigmoid: 1-4                                     [16, 3, 21, 72]           --\n",
      "====================================================================================================\n",
      "Total params: 123,119,875\n",
      "Trainable params: 123,119,875\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 229.75\n",
      "====================================================================================================\n",
      "Input size (MB): 0.29\n",
      "Forward/backward pass size (MB): 779.15\n",
      "Params size (MB): 492.48\n",
      "Estimated Total Size (MB): 1271.92\n",
      "====================================================================================================\n",
      "Total params: 123,119,875\n"
     ]
    }
   ],
   "source": [
    "model = UNet(\n",
    "    in_ch=n_channels, \n",
    "    out_ch=n_channels, \n",
    "    init_ch=hp.init_ch, \n",
    "    depth=hp.depth,\n",
    "    attention=hp.attention, \n",
    "    is_res=hp.is_res,\n",
    "    ).to(device)\n",
    "\n",
    "model_stats = summary(model, input_size=(hp.batch_size, *x_shape))\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model_stats)\n",
    "print(f'Total params: {total_params:,}')\n",
    "hp['total_params'] = total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Projects/mlma/model/MA-UNet/DLMA/wandb/run-20240419_145915-spgkgqm7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kinya/MA-UNet/runs/spgkgqm7' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/kinya/MA-UNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kinya/MA-UNet' target=\"_blank\">https://wandb.ai/kinya/MA-UNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kinya/MA-UNet/runs/spgkgqm7' target=\"_blank\">https://wandb.ai/kinya/MA-UNet/runs/spgkgqm7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, train: 0.00808,   0.576, val:  0.0077,   0.601\n",
      "Epoch   1, train: 0.00732,   0.559, val:  0.0075,   0.599\n",
      "Epoch   2, train: 0.00694,   0.549, val: 0.00719,   0.593\n",
      "Epoch   3, train: 0.00652,   0.539, val: 0.00693,   0.587\n",
      "Epoch   4, train: 0.00616,   0.527, val: 0.00672,   0.584\n",
      "Epoch   5, train: 0.00581,   0.514, val: 0.00706,   0.573\n",
      "Epoch   6, train: 0.00547,   0.495, val:  0.0064,   0.578\n",
      "Epoch   7, train: 0.00511,   0.470, val: 0.00631,   0.567\n",
      "Epoch   8, train: 0.00479,   0.443, val: 0.00628,   0.563\n",
      "Epoch   9, train: 0.00441,   0.408, val: 0.00608,   0.559\n",
      "Epoch  10, train: 0.00411,   0.378, val: 0.00606,   0.556\n",
      "Epoch  11, train: 0.00377,   0.344, val: 0.00595,   0.546\n",
      "Epoch  12, train: 0.00348,   0.316, val: 0.00596,   0.546\n",
      "Epoch  13, train: 0.00321,   0.291, val: 0.00594,   0.539\n",
      "Epoch  14, train:   0.003,   0.272, val: 0.00617,   0.535\n",
      "Epoch  15, train: 0.00282,   0.257, val: 0.00597,   0.528\n",
      "Epoch  16, train: 0.00265,   0.242, val: 0.00589,   0.528\n",
      "Epoch  17, train: 0.00251,   0.231, val: 0.00593,   0.523\n",
      "Epoch  18, train: 0.00237,   0.221, val: 0.00578,   0.529\n",
      "Epoch  19, train:  0.0023,   0.214, val: 0.00572,   0.537\n",
      "Epoch  20, train: 0.00218,   0.206, val: 0.00569,   0.528\n",
      "Epoch  21, train: 0.00208,   0.200, val: 0.00588,   0.528\n",
      "Epoch  22, train: 0.00201,   0.194, val: 0.00568,   0.526\n",
      "Epoch  23, train: 0.00193,   0.188, val: 0.00575,   0.525\n",
      "Epoch  24, train: 0.00186,   0.184, val: 0.00576,   0.526\n",
      "Epoch  25, train:  0.0018,   0.180, val: 0.00641,   0.523\n",
      "Epoch  26, train: 0.00176,   0.177, val: 0.00575,   0.526\n",
      "Epoch  27, train: 0.00172,   0.174, val: 0.00575,   0.526\n",
      "Epoch  28, train: 0.00168,   0.172, val: 0.00598,   0.516\n",
      "Epoch  29, train:  0.0016,   0.168, val: 0.00577,   0.520\n",
      "Epoch  30, train: 0.00156,   0.165, val: 0.00586,   0.520\n",
      "Epoch  31, train: 0.00153,   0.164, val: 0.00574,   0.524\n",
      "Epoch  32, train: 0.00149,   0.163, val: 0.00583,   0.520\n",
      "Epoch  33, train: 0.00148,   0.160, val: 0.00589,   0.517\n",
      "Epoch  34, train: 0.00144,   0.159, val: 0.00573,   0.529\n",
      "Epoch  35, train:  0.0014,   0.157, val: 0.00579,   0.518\n",
      "Epoch  36, train: 0.00138,   0.156, val: 0.00592,   0.514\n",
      "Epoch  37, train: 0.00135,   0.154, val: 0.00584,   0.522\n",
      "Epoch  38, train: 0.00133,   0.153, val: 0.00578,   0.517\n",
      "Epoch  39, train: 0.00132,   0.152, val: 0.00573,   0.525\n",
      "Epoch  40, train: 0.00129,   0.151, val: 0.00594,   0.517\n",
      "Epoch  41, train: 0.00127,   0.150, val: 0.00604,   0.519\n",
      "Epoch  42, train: 0.00125,   0.149, val: 0.00591,   0.517\n",
      "Epoch  43, train: 0.00123,   0.147, val: 0.00596,   0.516\n",
      "Epoch  44, train: 0.00121,   0.147, val: 0.00585,   0.516\n",
      "Epoch  45, train: 0.00119,   0.146, val: 0.00608,   0.520\n",
      "Epoch  46, train: 0.00118,   0.145, val: 0.00603,   0.518\n",
      "Epoch  47, train: 0.00116,   0.144, val: 0.00606,   0.516\n",
      "Epoch  48, train: 0.00116,   0.143, val: 0.00595,   0.517\n",
      "Epoch  49, train: 0.00114,   0.143, val:   0.006,   0.523\n",
      "Epoch  50, train: 0.00113,   0.142, val: 0.00602,   0.518\n",
      "Epoch  51, train: 0.00111,   0.141, val: 0.00603,   0.514\n",
      "Epoch  52, train:  0.0011,   0.141, val:  0.0061,   0.520\n",
      "Epoch  53, train: 0.00108,   0.140, val: 0.00616,   0.520\n",
      "Epoch  54, train: 0.00106,   0.139, val: 0.00612,   0.519\n",
      "Epoch  55, train: 0.00106,   0.139, val: 0.00627,   0.516\n",
      "Epoch  56, train: 0.00105,   0.138, val: 0.00623,   0.520\n",
      "Epoch  57, train: 0.00104,   0.138, val:  0.0063,   0.517\n",
      "Epoch  58, train: 0.00103,   0.137, val: 0.00609,   0.525\n",
      "Epoch  59, train: 0.00102,   0.137, val: 0.00616,   0.518\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6558829e7bd4dcc965101ccbd2b0133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mse</td><td>██▇▇▇▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▅▄▃▂▂▂▂▂▂▁▁▂▁▁▄▁▂▂▁▂▁▂▂▁▂▂▂▂▂▂▂▂▂▃▃▃▃</td></tr><tr><td>val_mse</td><td>██▇▇▆▅▅▄▄▃▂▂▂▃▂▂▂▂▂▁▁▂▁▂▁▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>51</td></tr><tr><td>best_mse</td><td>0.51367</td></tr><tr><td>train_loss</td><td>0.00102</td></tr><tr><td>train_mse</td><td>0.1368</td></tr><tr><td>val_loss</td><td>0.00616</td></tr><tr><td>val_mse</td><td>0.5176</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/kinya/MA-UNet/runs/spgkgqm7' target=\"_blank\">https://wandb.ai/kinya/MA-UNet/runs/spgkgqm7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240419_145915-spgkgqm7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 1min 36s, sys: 10.1 s, total: 1h 1min 47s\n",
      "Wall time: 1h 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project='MA-UNet', config=hp,\n",
    "    name=exp,\n",
    ")\n",
    "\n",
    "# Save hyperparameters\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# torch.save(hp, f'{out_dir}/hyperparameters.pt')\n",
    "with open(f'{out_dir}/hyperparameters.json', 'w') as f:\n",
    "    json.dump(hp, f)\n",
    "\n",
    "# Train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hp.learning_rate) \n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0.01, out_dir=out_dir)\n",
    "trainer = Trainer(\n",
    "    hp.n_epochs, model, device, optimizer, dataloaders,\n",
    "    t0_library, t0_mask, hp.n_sub, t1_library, hp.n_analog,\n",
    "    early_stopper,\n",
    ")\n",
    "history = trainer()\n",
    "\n",
    "# Save history\n",
    "history.astype('float').to_csv(f'{out_dir}/history.csv')\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with uniform weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import test_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0.00584,   0.794,   0.369\n",
      "val\n",
      " 0.0059,   0.834,   0.416\n",
      "test\n",
      "0.00601,   0.866,   0.449\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for key, dataloader in dataloaders.items():\n",
    "    print(key)\n",
    "\n",
    "    if key == 'train':\n",
    "        insample = True\n",
    "    else:\n",
    "        insample = False\n",
    "\n",
    "    loss, mean_mse, mse = test_uniform(\n",
    "        model, device, dataloader, \n",
    "        t0_library, t0_mask, hp.n_sub, t1_library,\n",
    "        n_analog=hp.n_analog, insample=insample,     \n",
    "    )\n",
    "\n",
    "    print(f'{loss:7.3g}, {mean_mse:7.3f}, {mse:7.3f}')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
